{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# Exploratory Data Analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Introduction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After the data cleaning step where we put our data into a few standard formats, the next step is to take a look at the data and see if what we're looking at makes sense. Before applying any fancy algorithms, it's always important to explore the data first.\n",
    "\n",
    "When working with numerical data, some of the exploratory data analysis (EDA) techniques we can use include finding the average of the data set, the distribution of the data, the most common values, etc. The idea is the same when working with text data. We are going to find some more obvious patterns with EDA before identifying the hidden patterns with machines learning (ML) techniques. We are going to look at the following for each comedian:\n",
    "\n",
    "1. **Most common words** - find these and create word clouds\n",
    "2. **Size of vocabulary** - look number of unique words and also how quickly someone speaks\n",
    "3. **Amount of profanity** - most common terms"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Assignments 1: \n",
    "#### Find `Most Common Words` and create word cloud."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Read in the document-term matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import necessary libraries\n",
    "import pandas as pd\n",
    "import pickle\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from wordcloud import WordCloud\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Load the Document-Term Matrix (dtm.pkl)\n",
    "data_dtm = pd.read_pickle(\"dtm.pkl\")\n",
    "\n",
    "# Load the cleaned data (data_clean.pkl)\n",
    "data_clean = pd.read_pickle(\"data_clean.pkl\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_dtm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Find  and print the top 30 words said by each comedian\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Find and print the top 30 words said by each comedian\n",
    "for comedian in data_dtm.index:\n",
    "    print(f\"\\nTop 30 words for {comedian}:\")\n",
    "    print(data_dtm.loc[comedian].sort_values(ascending=False).head(30))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### By looking at these top words, you can see that some of them have very little meaning and could be added to a stop words list, so let's do just that. Look at the most common top words and add them to the stop word list.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Stop words list for adel_karam\n",
    "stop_words_adel_karam = ['said', 'like', 'know', 'dont', 'dani','hes','youre', 'just', 'got', 'come', 'theres', 'want', 'im', 'thats']\n",
    "\n",
    "# Stop words list for amy_schumer\n",
    "stop_words_amy_schumer = ['like', 'im', 'just', 'know', 'guys', 'dont', 'right', 'thats', 'oh', 'theyre', 'youre', 'thank', 'cause', 'gonna', 'guy', 'okay', 'did', 'shes', 'think', 'yeah']\n",
    "\n",
    "# Stop words list for beth_stelling\n",
    "stop_words_beth_stelling = ['like', 'just', 'im', 'dont', 'know', 'youre', 'think', 'going', 'people', 'thats', 'feel', 'sex', 'theres', 'hes', 'got', 'theyre', 'maybe', 'mom', 'time', 'good']\n",
    "\n",
    "# Stop words list for big_jay_oakerson\n",
    "stop_words_big_jay_oakerson = ['like', 'just', 'im', 'know', 'thats', 'dont', 'right', 'youre', 'gonna', 'guy', 'shes', 'shit', 'got', 'man', 'dude', 'oh', 'big', 'good', 'yeah', 'guys']\n",
    "\n",
    "# Stop words list for chelsea_handler\n",
    "stop_words_chelsea_handler = ['like', 'im', 'said', 'dont', 'gonna', 'just', 'people', 'thats', 'youre', 'know', 'hes', 'going', 'time', 'didnt', 'okay', 'think', 'want', 'dan', 'cause', 'little']\n",
    "\n",
    "# Stop words list for chris_rock\n",
    "stop_words_chris_rock = ['like', 'thats', 'right', 'im', 'got', 'shit', 'know', 'okay', 'fuck', 'dont', 'just', 'man', 'cause', 'fucking', 'black', 'people', 'kids', 'theyre', 'everybody', 'white']\n",
    "\n",
    "# Stop words list for david_cross\n",
    "stop_words_david_cross = ['right', 'im', 'dont', 'thats', 'know', 'people', 'yeah', 'like', 'want', 'just', 'gonna', 'tell', 'think', 'hes', 'oh', 'god', 'thing', 'theres', 'theyre', 'okay']\n",
    "\n",
    "# Stop words list for dave_chappelle\n",
    "stop_words_dave_chappelle = ['know', 'im', 'like', 'said', 'man', 'tell', 'everybody', 'ngga', 'didnt', 'just', 'dont', 'time', 'right', 'shit', 'thats', 'got', 'dream', 'people', 'did', 'life']\n",
    "\n",
    "# Stop words list for dylan_moran\n",
    "stop_words_dylan_moran = ['know', 'people', 'just', 'thats', 'dont', 'going', 'im', 'look', 'youre', 'like', 'cause', 'time', 'really', 'oh', 'thing', 'didnt', 'need', 'theyre', 'okay', 'yeah']\n",
    "\n",
    "# Stop words list for george_carlin\n",
    "stop_words_george_carlin = ['like', 'im', 'know', 'youre', 'right', 'passed', 'guys', 'running', 'come', 'cloth', 'sergeant', 'ox', 'attack', 'life', 'loirn', 'limping', 'feather', 'dance', 'make', 'lot']\n",
    "\n",
    "# Stop words list for iliza_shlesinger\n",
    "stop_words_iliza_shlesinger = ['like', 'youre', 'im', 'dont', 'okay', 'know', 'just', 'right', 'thats', 'want', 'women', 'yeah', 'time', 'wedding', 'theyre', 'love', 'got', 'hes', 'going', 'think']\n",
    "\n",
    "# Stop words list for kevin_hart\n",
    "stop_words_kevin_hart = ['im', 'dont', 'fcking', 'said', 'got', 'know', 'thats', 'fck', 'like', 'man', 'just', 'right', 'shit', 'gonna', 'gotta', 'good', 'time', 'kids', 'house', 'people']\n",
    "\n",
    "# Stop words list for kevin_james\n",
    "stop_words_kevin_james = ['like', 'know', 'just', 'dont', 'im', 'thats', 'right', 'goes', 'youre', 'got', 'time', 'did', 'hes', 'good', 'yeah', 'shes', 'theyre', 'say', 'cause', 'gonna']\n",
    "\n",
    "# Stop words list for louis_c_k\n",
    "stop_words_louis_c_k = ['just', 'im', 'like', 'thats', 'dont', 'people', 'kids', 'want', 'shit', 'theres', 'know', 'youre', 'fuck', 'going', 'really', 'thing', 'good', 'time', 'theyre', 'kid']\n",
    "\n",
    "# Stop words list for matt_rife\n",
    "stop_words_matt_rife = ['like', 'know', 'im', 'just', 'thats', 'dont', 'fucking', 'oh', 'think', 'want', 'man', 'okay', 'people', 'youre', 'good', 'going', 'life', 'hes', 'time', 'fuck']\n",
    "\n",
    "# Stop words list for pete_davidson\n",
    "stop_words_pete_davidson = ['like', 'know', 'im', 'goes', 'dont', 'just', 'thats', 'mom', 'right', 'shes', 'youre', 'yeah', 'fuck', 'cause', 'fucking', 'got', 'okay', 'want', 'little', 'going']\n",
    "\n",
    "# Stop words list for ricky_gervais\n",
    "stop_words_ricky_gervais = ['just', 'like', 'dont', 'know', 'oh', 'right', 'im', 'got', 'okay', 'yeah', 'thats', 'fat', 'god', 'think', 'ive', 'people', 'going', 'theyre', 'said', 'fucking']\n",
    "\n",
    "# Stop words list for sarah_cooper\n",
    "stop_words_sarah_cooper = ['im', 'sarah', 'just', 'like', 'oh', 'cooper', 'nice', 'okay', 'yeah', 'gonna', 'youre', 'great', 'know', 'fine', 'good', 'got', 'right', 'thats', 'dont', 'little']\n",
    "\n",
    "# Stop words list for tom_segura\n",
    "stop_words_tom_segura = ['indians', 'indian', 'like', 'passed', 'everybody', 'im', 'know', 'youre', 'right', 'dont', 'goes', 'yeah', 'think', 'thats', 'say', 'want', 'got', 'gonna', 'shit', 'fucking']\n",
    "\n",
    "# Stop words list for trevor_noah\n",
    "stop_words_trevor_noah = ['like', 'know', 'dont', 'people', 'white', 'yeah', 'thats', 'youre', 'anthem', 'man', 'just', 'im', 'time', 'think', 'hes', 'germany', 'right', 'world', 'america', 'thing']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Let's aggregate this list and identify the most common words along with how many routines they occur in\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import Counter\n",
    "\n",
    "# Aggregate stop words lists\n",
    "all_stop_words = (\n",
    "    stop_words_adel_karam +\n",
    "    stop_words_amy_schumer +\n",
    "    stop_words_beth_stelling +\n",
    "    stop_words_big_jay_oakerson +\n",
    "    stop_words_chris_rock +\n",
    "    stop_words_chelsea_handler +\n",
    "    stop_words_david_cross +\n",
    "    stop_words_dave_chappelle +\n",
    "    stop_words_dylan_moran +\n",
    "    stop_words_george_carlin +\n",
    "    stop_words_iliza_shlesinger +\n",
    "    stop_words_kevin_hart +\n",
    "    stop_words_kevin_james +\n",
    "    stop_words_louis_c_k +\n",
    "    stop_words_matt_rife +\n",
    "    stop_words_pete_davidson +\n",
    "    stop_words_ricky_gervais +\n",
    "    stop_words_sarah_cooper +\n",
    "    stop_words_tom_segura +\n",
    "    stop_words_trevor_noah\n",
    ")\n",
    "\n",
    "# Count occurrences of each word\n",
    "word_counts = Counter(all_stop_words)\n",
    "\n",
    "# Identify the most common words and their occurrences\n",
    "most_common_words = word_counts.most_common()\n",
    "\n",
    "# Print the results\n",
    "print(\"Most common words and their occurrences:\")\n",
    "for word, count in most_common_words:\n",
    "    print(f\"{word}: {count} occurrences\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### If more than half of the comedians have it as a top word, exclude it from the list\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Identify comedians count\n",
    "num_comedians = 20\n",
    "\n",
    "# Exclude words that appear in more than half of the comedians' lists\n",
    "filtered_words = {word: count for word, count in word_counts.items() if count >= num_comedians / 2}\n",
    "\n",
    "# Print the filtered words\n",
    "print(\"Filtered words based on more than half comedians having it as a top word:\")\n",
    "for word, count in filtered_words.items():\n",
    "    print(f\"{word}: {count} occurrences\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Now I will put these stop words in Final_Stop_Words\n",
    "final_stop_words = list(set(filtered_words))\n",
    "\n",
    "print(final_stop_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's update our document-term matrix with the new list of stop words\n",
    "from sklearn.feature_extraction import text \n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "# Load the cleaned data (data_clean.pkl)\n",
    "data_clean = pd.read_pickle(\"data_clean.pkl\")\n",
    "\n",
    "stop_words = list(text.ENGLISH_STOP_WORDS.union(final_stop_words))\n",
    "# Add new stop words\n",
    "cv = CountVectorizer(stop_words=stop_words)\n",
    "\n",
    "# Recreate document-term matrix\n",
    "data_cv = cv.fit_transform(data_clean.transcript)\n",
    "data_dtm_NLP3 = pd.DataFrame(data_cv.toarray(), columns=cv.get_feature_names_out())\n",
    "data_dtm_NLP3.index = data_clean.index\n",
    "data_dtm_NLP3\n",
    "\n",
    "# Pickle it for later use\n",
    "with open('data_dtm_NLP3.pkl', 'wb') as file:\n",
    "    pickle.dump(data_dtm_NLP3, file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_dtm_NLP3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's make some word clouds!\n",
    "# Terminal / Anaconda Prompt: conda install -c conda-forge wordcloud\n",
    "# !pip install wordcloud\n",
    "from wordcloud import WordCloud"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reset the output dimensions\n",
    "plt.rcParams['figure.figsize'] = [16, 12]\n",
    "\n",
    "# Create subplots for each comedian\n",
    "comedians = data_dtm_NLP3.index\n",
    "for index, comedian in enumerate(comedians):\n",
    "    wc = WordCloud(stopwords=stop_words, background_color=\"white\", colormap=\"Dark2\", max_font_size=150, random_state=42)\n",
    "    wc.generate(data_clean.transcript[comedian])\n",
    "    \n",
    "    plt.subplot(4, 5, index+1)\n",
    "    plt.imshow(wc, interpolation=\"bilinear\")\n",
    "    plt.axis(\"off\")\n",
    "    plt.title(comedian)\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Observations\n",
    "1. George Carlin's top words are quite different, with specific references like \"indians,\" \"passed,\" and \"running,\" suggesting his tendency towards social commentary and observational humor. And its indicating the george carling is talking about an indian warrior/sergeant in his standup comedy\n",
    "\n",
    "2. Amy Schumer frequently uses words like \"guy\" \"girl\" and \"mom\" which might reflect that she might be talking about her relationships.\n",
    "\n",
    "3. Ricky Gervas is using words like \"fat\", \"gay\",\"god\" indicating that he's doing dark comedy either body shaming  and using homophobic comments\n",
    "\n",
    "4. Sarah Cooper's references to \"president\" and \"news,\" reflecting her political satire.\n",
    "\n",
    "Each comedian has a unique set of words that they frequently use, indicating their individual style and preferred topics."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Assignment 2:\n",
    "#### Find the number of unique words that each comedian uses."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To find unique comedians store I will data_dtm_nlp3 in a new dataframe then wherever there is numerical value I will change it to 1 and then to find which unique words are said by the  comedians that cell should be 1 and column sum should be 1 too "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a binary dataframe where each cell is 1 if the word is used by the comedian, 0 otherwise\n",
    "binary_data_dtm_NLP3 = data_dtm_NLP3.applymap(lambda x: 1 if x > 0 else 0)\n",
    "\n",
    "# Calculate the column sum for each word to check if it's unique to a comedian\n",
    "unique_words_sum = binary_data_dtm_NLP3.sum(axis=0)\n",
    "\n",
    "# Filter out words that are unique to a comedian (sum is 1)\n",
    "unique_words_per_comedian = binary_data_dtm_NLP3.loc[:, unique_words_sum == 1]\n",
    "\n",
    "# Calculate the row sum for each comedian to find the number of unique words\n",
    "number_of_unique_words_per_comedian = unique_words_per_comedian.sum(axis=1)\n",
    "\n",
    "# Create a new dataframe with the number of unique words per comedian\n",
    "unique_words_count_df = pd.DataFrame({'Comedian': number_of_unique_words_per_comedian.index, 'UniqueWordsCount': number_of_unique_words_per_comedian.values})\n",
    "\n",
    "# Print the dataframe with the number of unique words per comedian\n",
    "print(\"Number of Unique Words Said by Each Comedian:\")\n",
    "print(unique_words_count_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " #####  Before finding the wpm for each comedian i will total the number of words each comedian said"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Summing the values across each row to get the total words\n",
    "total_words_by_comedian = data_dtm.sum(axis=1)\n",
    "\n",
    "# Creating a new DataFrame df_nlp3\n",
    "df_nlp3 = pd.DataFrame({'Comedian': total_words_by_comedian.index, 'Total_Words': total_words_by_comedian.values})\n",
    "\n",
    "# Displaying the new DataFrame\n",
    "print(df_nlp3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "# List of comedians\n",
    "comedians = df_nlp3['Comedian'].tolist()\n",
    "# Took duration of each comedian from IMDB\n",
    "durations = [59,61,58,68,68,70,56,56,74,6,58,57,62,59,63,49,60,47,57,55]\n",
    "# Adding the 'Duration' column to the DataFrame\n",
    "df_nlp3['run_times'] = durations\n",
    "# Displaying the updated DataFrame\n",
    "print(df_nlp3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate the words per minute of each comedian\n",
    "df_nlp3['words_per_minute'] = df_nlp3['Total_Words'] / df_nlp3['run_times']\n",
    "df_nlp3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sorting the DataFrame by 'words_per_minute'\n",
    "df_sorted = df_nlp3.sort_values(by='words_per_minute', ascending=False)\n",
    "df_sorted"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Sorting the DataFrame by 'words_per_minute'\n",
    "df_sorted = df_nlp3.sort_values(by='words_per_minute', ascending=False)\n",
    "\n",
    "# Set up the subplots\n",
    "fig, axes = plt.subplots(1, 2, figsize=(12, 6))\n",
    "\n",
    "# Plotting the Number of Unique Words\n",
    "axes[0].barh(df_sorted['Comedian'], df_sorted['Total_Words'], color='skyblue')\n",
    "axes[0].set_yticks(df_sorted['Comedian'])\n",
    "axes[0].set_xlabel('Total Words')\n",
    "axes[0].set_title('Number of Total Words', fontsize=16)\n",
    "\n",
    "# Plotting the Number of Words Per Minute\n",
    "axes[1].barh(df_sorted['Comedian'], df_sorted['words_per_minute'], color='salmon')\n",
    "axes[1].set_yticks(df_sorted['Comedian'])\n",
    "axes[1].set_xlabel('Words Per Minute')\n",
    "axes[1].set_title('Number of Words Per Minute', fontsize=16)\n",
    "\n",
    "# Adjusting layout\n",
    "plt.tight_layout()\n",
    "\n",
    "# Show the plot\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Observations\n",
    "\n",
    "* **Talking Speed:**\n",
    "  * George Carlin and Kevin Hart speak quickly, averaging around 91 and 93 words per minute respectively.\n",
    "\n",
    "* **Vocabulary:**\n",
    "  * George Carlin has a smaller vocabulary, using only 550 words so it means that his comedy length was short, while Dylan Moran have expansive vocabularies, with over 5000 words.\n",
    "  \n",
    "* **Consistency in Talking Speed:**\n",
    "  * Matt rife and Dylan moran maintain steady talking speeds."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Assignment 3: \n",
    "#### Check the profanity by analysing the common bad words, like `fucking`, `fuck`, `shit etc."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# take a look at the most common words\n",
    "lol = Counter(data_dtm)\n",
    "most_common_words = [word for word, _ in lol.most_common()]\n",
    "print(most_common_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# isolate just these bad words\n",
    "profanity_words = ['fucking', 'fuck', 'shit','bitch','dickhead', 'dicks', 'dicksucker','fucked', 'fuckers', 'fuckin', 'fucking', 'fucks','ngga', 'nggas', 'ngger']\n",
    "data_bad_words = data_dtm[profanity_words]\n",
    "data_bad_words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pip install adjustText"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "from adjustText import adjust_text\n",
    "\n",
    "\n",
    "# Create a figure with subplots\n",
    "plt.figure(figsize=(6, 6))\n",
    "\n",
    "# Loop through each comedian\n",
    "texts = []\n",
    "for i, comedian in enumerate(data_bad_words.index):\n",
    "    # Filter 'f_words' and 's_words' based on starting letter\n",
    "    f_words = [word for word in profanity_words if word.startswith('f')]\n",
    "    s_words = [word for word in profanity_words if word.startswith('s')]\n",
    "\n",
    "    # Calculate the total count of 'f_words' and 's_words'\n",
    "    x = data_bad_words[f_words].sum(axis=1).loc[comedian]\n",
    "    y = data_bad_words[s_words].sum(axis=1).loc[comedian]\n",
    "\n",
    "    # Create a scatter plot\n",
    "    plt.scatter(x, y, color='blue')\n",
    "\n",
    "    # Add text annotations to the list\n",
    "    texts.append(plt.text(x + 1.5, y + 0.5, comedian, fontsize=8))\n",
    "\n",
    "# Adjust text positions to avoid overlapping\n",
    "adjust_text(texts, arrowprops=dict(arrowstyle='->', color='red'))\n",
    "\n",
    "# Set plot title and labels\n",
    "plt.title('Number of Bad Words Used in Routine', fontsize=20)\n",
    "plt.xlabel('Number of F Bombs', fontsize=15)\n",
    "plt.ylabel('Number of S Words', fontsize=15)\n",
    "\n",
    "# Show the plot\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "# Create a heatmap\n",
    "plt.figure(figsize=(12, 8))\n",
    "sns.heatmap(data_bad_words, annot=True, fmt=\"d\", cmap=\"YlGnBu\", xticklabels=profanity_words)\n",
    "plt.title(\"Profanity Words Count for Each Comedian\")\n",
    "plt.xlabel(\"Profanity Words\")\n",
    "plt.ylabel(\"Comedian Name\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Observations:\n",
    "\n",
    "Frequency of Profanity:\n",
    "\n",
    "- Matt Rife uses the most profanity overall, with high counts of \"fucking,\" \"fuck,\" and \"shit,\" suggesting a tendency towards more explicit language in his comedy.\n",
    "- Ricky Gervais also uses a significant amount of profanity, particularly the word \"fuck,\" along with variations like \"fucking\" and \"fucks.\"\n",
    "- George Carlin doesn't use any of the listed profane words, indicating a cleaner style of comedy.\n",
    "\n",
    "Variety of Profanity:\n",
    "\n",
    "- Chris Rock and Big Jay Oakerson use a wide range of profanity, including \"fucking,\" \"fuck,\" \"shit,\" and others, suggesting a more diverse use of explicit language in their routines.\n",
    "- Kevin Hart doesn't use the word \"fuck\" but does use variations like \"fucking\" and \"fucks,\" indicating a preference for slightly less explicit language.\n",
    "\n",
    "Use of Specific Profanity:\n",
    "\n",
    "- Pete Davidson and Louis C.K. use the word \"fuck\" most frequently, with high counts for \"fucking\" and \"fuckers,\" indicating a preference for this particular profanity in their comedy.\n",
    "- Chelsea Handler uses the word \"fuck\" and its variations frequently but doesn't use the word \"bitch\" as much as some other comedians, suggesting a focus on certain types of profanity over others.\n",
    "\n",
    "Absence of Profanity:\n",
    "\n",
    "- George Carlin and Kevin James don't use any of the listed profane words, indicating a clean or family-friendly style of comedy that avoids explicit language.\n",
    "- Sarah Cooper also refrains from using profanity, further indicating a cleaner comedic style."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Chris rock and Dave chappele have used the most number of N-words (Nigga) indicating that they are black skinned "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "### Assignment 4:(optional)\n",
    "What other word counts do you think would be interesting to compare instead of the f-word and s-word? Create a scatter plot comparing them."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ans: Other than plot f-word and s-word we can compare d-word and b-word"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "from adjustText import adjust_text\n",
    "\n",
    "\n",
    "# Create a figure with subplots\n",
    "plt.figure(figsize=(6, 6))\n",
    "\n",
    "# Loop through each comedian\n",
    "texts = []\n",
    "for i, comedian in enumerate(data_bad_words.index):\n",
    "    # Filter 'f_words' and 's_words' based on starting letter\n",
    "    f_words = [word for word in profanity_words if word.startswith('b')]\n",
    "    s_words = [word for word in profanity_words if word.startswith('d')]\n",
    "\n",
    "    # Calculate the total count of 'f_words' and 's_words'\n",
    "    x = data_bad_words[f_words].sum(axis=1).loc[comedian]\n",
    "    y = data_bad_words[s_words].sum(axis=1).loc[comedian]\n",
    "\n",
    "    # Create a scatter plot\n",
    "    plt.scatter(x, y, color='blue')\n",
    "\n",
    "    # Add text annotations to the list\n",
    "    texts.append(plt.text(x + 1.5, y + 0.5, comedian, fontsize=8))\n",
    "\n",
    "# Adjust text positions to avoid overlapping\n",
    "adjust_text(texts, arrowprops=dict(arrowstyle='->', color='red'))\n",
    "\n",
    "# Set plot title and labels\n",
    "plt.title('Number of Bad Words Used in Routine', fontsize=20)\n",
    "plt.xlabel('Number of B Bombs', fontsize=15)\n",
    "plt.ylabel('Number of D Words', fontsize=15)\n",
    "\n",
    "# Show the plot\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "from adjustText import adjust_text\n",
    "\n",
    "\n",
    "# Create a figure with subplots\n",
    "plt.figure(figsize=(6, 6))\n",
    "\n",
    "# Loop through each comedian\n",
    "texts = []\n",
    "for i, comedian in enumerate(data_bad_words.index):\n",
    "    # Filter 'f_words' and 's_words' based on starting letter\n",
    "    f_words = [word for word in profanity_words if word.startswith('f')]\n",
    "    s_words = [word for word in profanity_words if word.startswith('d')]\n",
    "\n",
    "    # Calculate the total count of 'f_words' and 's_words'\n",
    "    x = data_bad_words[f_words].sum(axis=1).loc[comedian]\n",
    "    y = data_bad_words[s_words].sum(axis=1).loc[comedian]\n",
    "\n",
    "    # Create a scatter plot\n",
    "    plt.scatter(x, y, color='blue')\n",
    "\n",
    "    # Add text annotations to the list\n",
    "    texts.append(plt.text(x + 1.5, y + 0.5, comedian, fontsize=8))\n",
    "\n",
    "# Adjust text positions to avoid overlapping\n",
    "adjust_text(texts, arrowprops=dict(arrowstyle='->', color='red'))\n",
    "\n",
    "# Set plot title and labels\n",
    "plt.title('Number of Bad Words Used in Routine', fontsize=20)\n",
    "plt.xlabel('Number of F Bombs', fontsize=15)\n",
    "plt.ylabel('Number of D Words', fontsize=15)\n",
    "\n",
    "# Show the plot\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "from adjustText import adjust_text\n",
    "\n",
    "\n",
    "# Create a figure with subplots\n",
    "plt.figure(figsize=(6, 6))\n",
    "\n",
    "# Loop through each comedian\n",
    "texts = []\n",
    "for i, comedian in enumerate(data_bad_words.index):\n",
    "    # Filter 'f_words' and 's_words' based on starting letter\n",
    "    f_words = [word for word in profanity_words if word.startswith('b')]\n",
    "    s_words = [word for word in profanity_words if word.startswith('f')]\n",
    "\n",
    "    # Calculate the total count of 'f_words' and 's_words'\n",
    "    x = data_bad_words[f_words].sum(axis=1).loc[comedian]\n",
    "    y = data_bad_words[s_words].sum(axis=1).loc[comedian]\n",
    "\n",
    "    # Create a scatter plot\n",
    "    plt.scatter(x, y, color='blue')\n",
    "\n",
    "    # Add text annotations to the list\n",
    "    texts.append(plt.text(x + 1.5, y + 0.5, comedian, fontsize=8))\n",
    "\n",
    "# Adjust text positions to avoid overlapping\n",
    "adjust_text(texts, arrowprops=dict(arrowstyle='->', color='red'))\n",
    "\n",
    "# Set plot title and labels\n",
    "plt.title('Number of Bad Words Used in Routine', fontsize=20)\n",
    "plt.xlabel('Number of B Bombs', fontsize=15)\n",
    "plt.ylabel('Number of F Words', fontsize=15)\n",
    "\n",
    "# Show the plot\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "from adjustText import adjust_text\n",
    "\n",
    "\n",
    "# Create a figure with subplots\n",
    "plt.figure(figsize=(6, 6))\n",
    "\n",
    "# Loop through each comedian\n",
    "texts = []\n",
    "for i, comedian in enumerate(data_bad_words.index):\n",
    "    # Filter 'f_words' and 's_words' based on starting letter\n",
    "    f_words = [word for word in profanity_words if word.startswith('f')]\n",
    "    s_words = [word for word in profanity_words if word.startswith('n')]\n",
    "\n",
    "    # Calculate the total count of 'f_words' and 's_words'\n",
    "    x = data_bad_words[f_words].sum(axis=1).loc[comedian]\n",
    "    y = data_bad_words[s_words].sum(axis=1).loc[comedian]\n",
    "\n",
    "    # Create a scatter plot\n",
    "    plt.scatter(x, y, color='blue')\n",
    "\n",
    "    # Add text annotations to the list\n",
    "    texts.append(plt.text(x + 1.5, y + 0.5, comedian, fontsize=8))\n",
    "\n",
    "# Adjust text positions to avoid overlapping\n",
    "adjust_text(texts, arrowprops=dict(arrowstyle='->', color='red'))\n",
    "\n",
    "# Set plot title and labels\n",
    "plt.title('Number of Bad Words Used in Routine', fontsize=20)\n",
    "plt.xlabel('Number of F Bombs', fontsize=15)\n",
    "plt.ylabel('Number of N Words', fontsize=15)\n",
    "\n",
    "# Show the plot\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate total f-bomb frequency for each comedian\n",
    "data_bad_words['total_f_bomb'] = data_bad_words.filter(like='f').sum(axis=1)\n",
    "\n",
    "# Plotting\n",
    "plt.figure(figsize=(12, 8))\n",
    "plt.bar(data_bad_words.index, data_bad_words['total_f_bomb'], color='skyblue')\n",
    "plt.xlabel('Comedian')\n",
    "plt.ylabel('Frequency of F-Bombs')\n",
    "plt.title('Frequency of F-Bombs by Comedian')\n",
    "plt.xticks(rotation=90)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  },
  "toc": {
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": "block",
   "toc_window_display": false
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
